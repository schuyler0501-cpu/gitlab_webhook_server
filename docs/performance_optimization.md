# 性能优化文档

## 🚀 并发优化

### 工作池（Worker Pool）

项目使用工作池模式处理 Webhook 任务，提高并发处理能力。

**配置**：
```env
WORKER_POOL_WORKERS=10      # 工作协程数量
WORKER_POOL_QUEUE_SIZE=1000 # 任务队列大小
```

**特性**：
- 可配置的工作协程数量
- 任务队列缓冲，避免任务丢失
- 自动重试机制（默认 3 次）
- 优雅关闭，确保任务完成

### 异步处理

Webhook 处理采用异步模式：

1. **立即响应**：收到 Webhook 后立即返回 202 Accepted
2. **后台处理**：使用 goroutine 异步处理
3. **任务队列**：提交到工作池队列处理
4. **降级策略**：队列满时降级为同步处理

### 限流保护

**配置**：
```env
RATE_LIMIT=100           # 限制数量
RATE_LIMIT_WINDOW=1m    # 时间窗口
```

**特性**：
- 基于 IP 的限流
- 滑动时间窗口
- 自动清理过期记录
- 返回 429 Too Many Requests

## 📊 数据库优化

### 连接池

- 最大空闲连接：10
- 最大打开连接：100
- 自动管理连接生命周期

### 事务处理

- 使用 GORM 事务保证数据一致性
- 批量操作使用事务
- 失败时自动回滚

### 索引优化

数据库表已创建必要的索引：
- `commits`: author_email, timestamp, project_name, commit_id
- `commit_files`: commit_id, language, change_type
- `commit_languages`: commit_id, language

## 🔄 历史数据导入

### GitLab API 客户端

使用 `github.com/xanzy/go-gitlab` 库访问 GitLab API。

**功能**：
- 获取项目提交记录（支持分页）
- 获取提交 diff（包含行数统计）
- 获取项目信息

### 导入策略

1. **分页获取**：避免一次性加载大量数据
2. **批量处理**：每批处理 100 条记录
3. **速率控制**：请求间隔 100ms，避免 API 限流
4. **错误处理**：单个提交失败不影响整体导入
5. **去重处理**：自动跳过已存在的提交记录

### 使用方式

**导入项目提交记录**：
```bash
curl -X POST http://localhost:3000/api/import/project \
  -H "Content-Type: application/json" \
  -d '{
    "project_id": "123",
    "since": "2024-01-01T00:00:00Z",
    "until": "2024-12-31T23:59:59Z",
    "batch_size": 100
  }'
```

**参数说明**：
- `project_id`: GitLab 项目 ID（必需）
- `since`: 开始时间（可选，RFC3339 格式）
- `until`: 结束时间（可选，RFC3339 格式）
- `batch_size`: 批次大小（可选，默认 100）

## 📈 性能指标

### 预期性能

- **Webhook 处理**：< 100ms 响应时间
- **并发处理**：支持 100+ 并发 Webhook
- **导入速度**：约 10-20 提交/秒（受 GitLab API 限制）

### 监控建议

1. **工作池监控**：
   - 队列长度
   - 任务处理速度
   - 失败率

2. **数据库监控**：
   - 连接池使用率
   - 查询响应时间
   - 事务成功率

3. **API 监控**：
   - 请求响应时间
   - 限流触发次数
   - 错误率

## 🛠️ 调优建议

### 高并发场景

1. **增加工作池大小**：
   ```env
   WORKER_POOL_WORKERS=20
   WORKER_POOL_QUEUE_SIZE=2000
   ```

2. **调整限流参数**：
   ```env
   RATE_LIMIT=200
   RATE_LIMIT_WINDOW=1m
   ```

3. **数据库连接池**：
   - 根据实际负载调整连接数
   - 监控连接池使用情况

### 大量历史数据导入

1. **分批导入**：
   - 按时间范围分批导入
   - 避免一次性导入过多数据

2. **调整批次大小**：
   - 根据网络和 API 限制调整
   - 建议 50-100 条/批

3. **监控导入进度**：
   - 查看日志了解导入状态
   - 检查数据库记录数量

## ⚠️ 注意事项

1. **GitLab API 限制**：
   - 注意 GitLab API 的速率限制
   - 导入时自动控制请求频率

2. **数据库性能**：
   - 大量导入时可能影响数据库性能
   - 建议在低峰期进行大批量导入

3. **内存使用**：
   - 工作池队列会占用内存
   - 根据实际情况调整队列大小

4. **错误处理**：
   - 单个任务失败不影响其他任务
   - 查看日志了解失败原因

